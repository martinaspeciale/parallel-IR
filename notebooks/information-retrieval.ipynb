{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd9630f",
   "metadata": {},
   "source": [
    "# ParallelIR\n",
    "\n",
    "### Authors: Filippo Lucchesi, Francesco Pio Crispino, Martina Speciale\n",
    "\n",
    "#### Pulp Fiction Group\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Project Overview\n",
    "\n",
    "This project implements a modular and parallelized **Information Retrieval (IR)** system, developed as part of an academic lab.\n",
    "\n",
    "The main objectives include:\n",
    "- Efficient **parallel construction** of the inverted index\n",
    "- Comparison of ranking functions: **TF-IDF vs BM25**\n",
    "- Use of **caching** to optimize repeated queries\n",
    "- Implementation of a custom **Relevance Feedback** algorithm inspired by Rocchio\n",
    "\n",
    "All experiments are run and benchmarked using the [`python-terrier`](https://github.com/terrier-org/pyterrier) framework and the [IR Datasets](https://ir-datasets.com/) library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01e7e6",
   "metadata": {},
   "source": [
    "### üîÅ Load previously saved data\n",
    "##### üíæ Efficient Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a979960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load all objects from disk\n",
    "docs = joblib.load(\"cache/docs.joblib\")\n",
    "doc_ids = joblib.load(\"cache/doc_ids.joblib\")\n",
    "topics_df = joblib.load(\"cache/topics_df.joblib\")\n",
    "qrels_df = joblib.load(\"cache/qrels_df.joblib\")\n",
    "tfidf_matrix = joblib.load(\"cache/tfidf_matrix.joblib\")\n",
    "tfidf_vectorizer = joblib.load(\"cache/tfidf_vectorizer.joblib\")\n",
    "\n",
    "# Try loading the cache, or fall back to empty\n",
    "try:\n",
    "    query_cache = joblib.load(\"cache/query_cache.joblib\")\n",
    "except FileNotFoundError:\n",
    "    query_cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c8a4e",
   "metadata": {},
   "source": [
    "## üì¶ Environment Setup\n",
    "\n",
    "We install all required Python libraries and handle NLTK downloads. This notebook is designed to run on **Kaggle** (GPU optional).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ae03be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/martina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/martina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install required packages (only needed once per environment)\n",
    "%pip install -q ir_datasets ir-measures scikit-learn dill pybind11 tqdm pympler python-terrier\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK resources (only the first time)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746bc8a8",
   "metadata": {},
   "source": [
    "## üìö Imports and PyTerrier Setup\n",
    "\n",
    "We now import all core libraries for Information Retrieval, ranking, analysis, and visualization. PyTerrier is used for document indexing, ranking, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c32c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IR and evaluation\n",
    "import pyterrier as pt\n",
    "import ir_datasets\n",
    "import ir_measures\n",
    "from ir_measures import *\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utility libraries\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import heapq\n",
    "import hashlib\n",
    "import string\n",
    "import array\n",
    "import collections\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ed9bd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java started and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Initialize PyTerrier (run once per session)\n",
    "if not pt.java.started():\n",
    "    pt.java.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a736e8",
   "metadata": {},
   "source": [
    "## üìÑ Dataset and Indexing\n",
    "\n",
    "We now load the IR dataset using `ir_datasets` and prepare it for use with PyTerrier by indexing its documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q ir_datasets ir-measures scikit-learn dill pybind11 tqdm pympler python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c316e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: Dataset(id='antique/train', provides=['docs', 'queries', 'qrels'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 403666\n",
      "Queries: 2426\n",
      "Qrels (relevance judgments): 27422\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset using ir_datasets\n",
    "dataset = ir_datasets.load(\"antique/train\")\n",
    "\n",
    "# Print basic dataset info\n",
    "print(\"Dataset loaded:\", dataset)\n",
    "print(\"Documents:\", dataset.docs_count())\n",
    "print(\"Queries:\", dataset.queries_count())\n",
    "print(\"Qrels (relevance judgments):\", dataset.qrels_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7969ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory one level above notebooks\n",
    "import os\n",
    "os.makedirs(\"../indexes\", exist_ok=True)\n",
    "\n",
    "# Set path for index\n",
    "index_path = \"../indexes/antique-index\"\n",
    "\n",
    "# Build the index if it doesn't already exist\n",
    "if not os.path.exists(os.path.join(index_path, \"data.properties\")):\n",
    "    indexer = pt.IterDictIndexer(index_path)\n",
    "    indexref = indexer.index(\n",
    "        ({\"docno\": doc.doc_id, \"text\": doc.text} for doc in dataset.docs_iter())\n",
    "    )\n",
    "else:\n",
    "    indexref = pt.IndexRef.of(index_path)\n",
    "\n",
    "# Load the index\n",
    "index = pt.IndexFactory.of(indexref)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd437b9d",
   "metadata": {},
   "source": [
    "## üîç Retrieval: TF-IDF and BM25\n",
    "\n",
    "We now create two retrieval pipelines: one based on the TF-IDF weighting scheme, and one on BM25. These are evaluated on the Vaswani dataset using standard IR metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2cfaf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9434/422261853.py:2: DeprecationWarning: Call to deprecated class BatchRetrieve. (use pt.terrier.Retriever() instead) -- Deprecated since version 0.11.0.\n",
      "  bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
      "/tmp/ipykernel_9434/422261853.py:3: DeprecationWarning: Call to deprecated class BatchRetrieve. (use pt.terrier.Retriever() instead) -- Deprecated since version 0.11.0.\n",
      "  tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample queries:\n",
      "  query_id                                               text\n",
      "0  3097310  What causes severe swelling and pain in the kn...\n",
      "1  3910705  why don't they put parachutes underneath airpl...\n",
      "2   237390                how to clean alloy cylinder heads ?\n",
      "3  2247892                          how do i get them whiter?\n",
      "4  1078492                    What is Cloud 9 and 7th Heaven?\n",
      "Sample qrels:\n",
      "  query_id     doc_id  relevance iteration\n",
      "0  2531329  2531329_0          4        U0\n",
      "1  2531329  2531329_5          4        Q0\n",
      "2  2531329  2531329_4          3        Q0\n",
      "3  2531329  2531329_7          3        Q0\n",
      "4  2531329  2531329_6          3        Q0\n"
     ]
    }
   ],
   "source": [
    "# Create BM25 and TF-IDF retrieval pipelines\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "\n",
    "# Load queries and qrels (ground-truth relevance judgments)\n",
    "topics = dataset.queries_iter()\n",
    "qrels = dataset.qrels_iter()\n",
    "\n",
    "# Convert queries and qrels to DataFrames for use with PyTerrier\n",
    "topics_df = pd.DataFrame([t._asdict() for t in topics])\n",
    "qrels_df = pd.DataFrame([q._asdict() for q in qrels])\n",
    "\n",
    "# Preview query format\n",
    "print(\"Sample queries:\")\n",
    "print(topics_df.head())\n",
    "print(\"Sample qrels:\")\n",
    "print(qrels_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f3c0c",
   "metadata": {},
   "source": [
    "### üõ† Column Mapping for PyTerrier Compatibility\n",
    "\n",
    "To use custom `topics` and `qrels` DataFrames with `pt.Experiment`, you must rename the columns to match what PyTerrier expects:\n",
    "\n",
    "| Original Column | Renamed To | Reason                        |\n",
    "|------------------|-------------|-------------------------------|\n",
    "| `query_id`       | `qid`       | PyTerrier expects this field |\n",
    "| `text`           | `query`     | PyTerrier expects this field |\n",
    "| `doc_id`         | `docno`     | Matches index document field |\n",
    "| `relevance`      | `label`     | Required as relevance score  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f59732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert iterators to DataFrames\n",
    "topics_df = pd.DataFrame(dataset.queries_iter())\n",
    "qrels_df = pd.DataFrame(dataset.qrels_iter())\n",
    "\n",
    "# Rename only if necessary\n",
    "topics_df = topics_df.rename(columns={k: v for k, v in {\n",
    "    \"query_id\": \"qid\",\n",
    "    \"text\": \"query\"\n",
    "}.items() if k in topics_df.columns})\n",
    "\n",
    "qrels_df = qrels_df.rename(columns={k: v for k, v in {\n",
    "    \"query_id\": \"qid\",\n",
    "    \"doc_id\": \"docno\",\n",
    "    \"relevance\": \"label\"\n",
    "}.items() if k in qrels_df.columns})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d491ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       qid                                              query\n",
      "0  3097310  What causes severe swelling and pain in the knees\n",
      "1  3910705  why dont they put parachutes underneath airpla...\n",
      "2   237390                 how to clean alloy cylinder heads \n",
      "       qid      docno  label iteration\n",
      "0  2531329  2531329_0      4        U0\n",
      "1  2531329  2531329_5      4        Q0\n",
      "2  2531329  2531329_4      3        Q0\n"
     ]
    }
   ],
   "source": [
    "# Preprocess queries: strip punctuation that breaks Terrier parser\n",
    "topics_df[\"query\"] = topics_df[\"query\"].str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
    "\n",
    "# Optional preview\n",
    "print(topics_df.head(3))\n",
    "print(qrels_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7bd7845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>R@5</th>\n",
       "      <th>R@10</th>\n",
       "      <th>R@15</th>\n",
       "      <th>R@20</th>\n",
       "      <th>R@30</th>\n",
       "      <th>R@100</th>\n",
       "      <th>R@200</th>\n",
       "      <th>R@500</th>\n",
       "      <th>R@1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.134620</td>\n",
       "      <td>0.285444</td>\n",
       "      <td>0.124418</td>\n",
       "      <td>0.159724</td>\n",
       "      <td>0.182997</td>\n",
       "      <td>0.200185</td>\n",
       "      <td>0.222592</td>\n",
       "      <td>0.292359</td>\n",
       "      <td>0.336036</td>\n",
       "      <td>0.395777</td>\n",
       "      <td>0.439749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.134642</td>\n",
       "      <td>0.285791</td>\n",
       "      <td>0.124607</td>\n",
       "      <td>0.160150</td>\n",
       "      <td>0.183631</td>\n",
       "      <td>0.200644</td>\n",
       "      <td>0.223307</td>\n",
       "      <td>0.293343</td>\n",
       "      <td>0.336536</td>\n",
       "      <td>0.396537</td>\n",
       "      <td>0.441141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name       map      ndcg       R@5      R@10      R@15      R@20  \\\n",
       "0  TF-IDF  0.134620  0.285444  0.124418  0.159724  0.182997  0.200185   \n",
       "1    BM25  0.134642  0.285791  0.124607  0.160150  0.183631  0.200644   \n",
       "\n",
       "       R@30     R@100     R@200     R@500    R@1000  \n",
       "0  0.222592  0.292359  0.336036  0.395777  0.439749  \n",
       "1  0.223307  0.293343  0.336536  0.396537  0.441141  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyterrier.measures import *\n",
    "\n",
    "results = pt.Experiment(\n",
    "    [tfidf, bm25],\n",
    "    topics_df,\n",
    "    qrels_df,\n",
    "    eval_metrics=[\"map\", \"ndcg\", \"recall\"],\n",
    "    names=[\"TF-IDF\", \"BM25\"]\n",
    ")\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e559b014",
   "metadata": {},
   "source": [
    "## üìä Retrieval Performance on ANTIQUE/test\n",
    "\n",
    "We evaluated two classic retrieval models ‚Äî **TF-IDF** and **BM25** ‚Äî on the ANTIQUE/test dataset using PyTerrier. Both models were run against the full document collection, and their performance was measured using standard IR metrics.\n",
    "\n",
    "### üîç Evaluation Metrics\n",
    "\n",
    "| Model   | MAP      | nDCG     | R@5     | R@10    | R@15    | R@20    | R@30    | R@100   | R@200   | R@500   | R@1000  |\n",
    "|---------|----------|----------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n",
    "| TF-IDF  | 0.134620 | 0.285444 | 0.12442 | 0.15972 | 0.18300 | 0.20018 | 0.22259 | 0.29236 | 0.33604 | 0.39578 | 0.43975 |\n",
    "| BM25    | 0.134642 | 0.285791 | 0.12461 | 0.16015 | 0.18363 | 0.20064 | 0.22331 | 0.29334 | 0.33654 | 0.39654 | 0.44114 |\n",
    "\n",
    "### üìå Observations\n",
    "\n",
    "- **BM25 slightly outperforms TF-IDF** across all metrics, particularly on deep recall levels like R@1000.\n",
    "- The performance difference is **marginal**, indicating both models behave similarly on this dataset.\n",
    "- These scores reflect the difficulty of **open-ended natural language queries** in the ANTIQUE dataset ‚Äî improvements would likely require semantic models (e.g., BERT-based retrieval).\n",
    "\n",
    "These baseline results provide a foundation for future comparison with more advanced neural or hybrid ranking approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a83a7",
   "metadata": {},
   "source": [
    "## üß± Manual Construction of TF-IDF Matrix and Cosine Similarity Ranking\n",
    "\n",
    "In this section, we manually implement a basic information retrieval system based on:\n",
    "\n",
    "- **TF-IDF vectorization** of the documents\n",
    "- **Cosine similarity** calculation between a query and all documents\n",
    "- Ranking documents by their similarity to the query\n",
    "\n",
    "This approach helps us understand the core mechanics of term-based retrieval models, without relying on built-in PyTerrier components like BM25 or TF.\n",
    "\n",
    "The implementation uses:\n",
    "- `TfidfVectorizer` from `sklearn` to transform documents into vector space\n",
    "- `cosine_similarity` to compute the similarity between the query and each document\n",
    "- Result sorting to return the top-K most similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd962680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (403666, 186364)\n"
     ]
    }
   ],
   "source": [
    "# Setup and Manual TF-IDF Construction\n",
    "\n",
    "# Required libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Extract documents and document IDs from the dataset\n",
    "docs = [doc.text for doc in dataset.docs_iter()]\n",
    "doc_ids = [doc.doc_id for doc in dataset.docs_iter()]\n",
    "\n",
    "# Create the TF-IDF matrix for all documents\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1a80d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What causes severe swelling and pain in the knees\n",
      "1. doc_id: 3786595_4 - score: 0.5125\n",
      "2. doc_id: 2606613_7 - score: 0.4862\n",
      "3. doc_id: 2859959_18 - score: 0.4793\n",
      "4. doc_id: 2933555_0 - score: 0.4529\n",
      "5. doc_id: 532973_9 - score: 0.4466\n",
      "6. doc_id: 389820_27 - score: 0.4094\n",
      "7. doc_id: 3786595_5 - score: 0.3985\n",
      "8. doc_id: 637192_11 - score: 0.3899\n",
      "9. doc_id: 3704893_18 - score: 0.3867\n",
      "10. doc_id: 3301833_0 - score: 0.3852\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity Ranking (Single Query)\n",
    "\n",
    "# Pick a query to test (e.g., first query in the dataset)\n",
    "sample_query = topics_df.iloc[0][\"query\"]\n",
    "\n",
    "# Transform the query using the same TF-IDF vectorizer\n",
    "query_vec = tfidf_vectorizer.transform([sample_query])\n",
    "\n",
    "# Compute cosine similarity between the query vector and document matrix\n",
    "cos_sim = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "\n",
    "# Get top 10 most similar documents\n",
    "top_n = 10\n",
    "top_doc_indices = np.argsort(cos_sim)[::-1][:top_n]\n",
    "\n",
    "# Print ranked document results\n",
    "print(\"Query:\", sample_query)\n",
    "for rank, idx in enumerate(top_doc_indices):\n",
    "    print(f\"{rank + 1}. doc_id: {doc_ids[idx]} - score: {cos_sim[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f9b814",
   "metadata": {},
   "source": [
    "## ‚ö° Optimization: Caching Cosine Similarity Results\n",
    "\n",
    "To improve efficiency, we introduce a basic **caching mechanism**:\n",
    "\n",
    "- Each query is saved in a dictionary (`query_cache`) along with its ranked results\n",
    "- If the same query is submitted again, the results are fetched directly from the cache instead of recalculating them\n",
    "- This significantly reduces execution time in interactive or repeated-query scenarios\n",
    "\n",
    "The function `retrieve_with_cache()` encapsulates this logic:\n",
    "- If the query exists in `query_cache`, cached results are returned\n",
    "- Otherwise, similarity is computed and stored for future use\n",
    "\n",
    "This optimization is especially useful in experiments with large document collections or when evaluating many repeated queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4908d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° Caching Similarity Results\n",
    "\n",
    "# Create a dictionary to store cached results\n",
    "query_cache = {}\n",
    "\n",
    "# Function that checks cache or computes cosine similarity\n",
    "def retrieve_with_cache(query_text, top_k=10):\n",
    "    # Return cached result if available\n",
    "    if query_text in query_cache:\n",
    "        return query_cache[query_text]\n",
    "\n",
    "    # Otherwise, compute similarity and cache the result\n",
    "    query_vec = tfidf_vectorizer.transform([query_text])\n",
    "    cos_sim = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "    top_doc_indices = np.argsort(cos_sim)[::-1][:top_k]\n",
    "\n",
    "    results = [(doc_ids[i], cos_sim[i]) for i in top_doc_indices]\n",
    "    query_cache[query_text] = results\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2909c1d1",
   "metadata": {},
   "source": [
    "### üíæ Save to Disk\n",
    "##### Efficient Reproducibility: Caching Intermediate Objects\n",
    "\n",
    "* To improve reproducibility and avoid re-running expensive operations, we serialize and store key data structures using `joblib`. All serialized files are saved into a dedicated `cache/` directory.\n",
    "\n",
    "#### üì¶ What We Save\n",
    "\n",
    "| Object            | Description                                      |\n",
    "|-------------------|--------------------------------------------------|\n",
    "| `docs`            | List of raw document texts from the dataset      |\n",
    "| `doc_ids`         | Corresponding document identifiers               |\n",
    "| `topics_df`       | DataFrame of queries (renamed for PyTerrier)     |\n",
    "| `qrels_df`        | DataFrame of relevance judgments                 |\n",
    "| `tfidf_matrix`    | The TF-IDF representation of the documents       |\n",
    "| `tfidf_vectorizer`| The fitted `TfidfVectorizer` object              |\n",
    " | `query_cache`     | (Optional) A dictionary caching query results    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f53bd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cache/tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "# Create the cache directory\n",
    "os.makedirs(\"cache\", exist_ok=True)\n",
    "\n",
    "# Save base dataset objects\n",
    "joblib.dump(docs, \"cache/docs.joblib\")\n",
    "joblib.dump(doc_ids, \"cache/doc_ids.joblib\")\n",
    "joblib.dump(topics_df, \"cache/topics_df.joblib\")\n",
    "joblib.dump(qrels_df, \"cache/qrels_df.joblib\")\n",
    "\n",
    "# Save TF-IDF matrix and vectorizer\n",
    "joblib.dump(tfidf_matrix, \"cache/tfidf_matrix.joblib\")\n",
    "joblib.dump(tfidf_vectorizer, \"cache/tfidf_vectorizer.joblib\")\n",
    "\n",
    "# Save query cache (optional)\n",
    "joblib.dump(query_cache, \"cache/query_cache.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
