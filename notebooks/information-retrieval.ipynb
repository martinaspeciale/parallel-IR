{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd9630f",
   "metadata": {},
   "source": [
    "# ParallelIR\n",
    "\n",
    "### Authors: Filippo Lucchesi, Francesco Pio Crispino, Martina Speciale\n",
    "\n",
    "#### Pulp Fiction Group\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Project Overview\n",
    "\n",
    "This project implements a modular and parallelized **Information Retrieval (IR)** system, developed as part of an academic lab.\n",
    "\n",
    "The main objectives include:\n",
    "- Efficient **parallel construction** of the inverted index\n",
    "- Comparison of ranking functions: **TF-IDF vs BM25**\n",
    "- Use of **caching** to optimize repeated queries\n",
    "- Implementation of a custom **Relevance Feedback** algorithm inspired by Rocchio\n",
    "\n",
    "All experiments are run and benchmarked using the [`python-terrier`](https://github.com/terrier-org/pyterrier) framework and the [IR Datasets](https://ir-datasets.com/) library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c8a4e",
   "metadata": {},
   "source": [
    "## üì¶ Environment Setup\n",
    "\n",
    "We install all required Python libraries and handle NLTK downloads. This notebook is designed to run on **Kaggle** (GPU optional).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ae03be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/martina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/martina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install required packages (only needed once per environment)\n",
    "!pip install -q ir_datasets ir-measures scikit-learn dill pybind11 tqdm pympler python-terrier\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK resources (only the first time)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746bc8a8",
   "metadata": {},
   "source": [
    "## üìö Imports and PyTerrier Setup\n",
    "\n",
    "We now import all core libraries for Information Retrieval, ranking, analysis, and visualization. PyTerrier is used for document indexing, ranking, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c32c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IR and evaluation\n",
    "import pyterrier as pt\n",
    "import ir_datasets\n",
    "import ir_measures\n",
    "from ir_measures import *\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utility libraries\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import heapq\n",
    "import hashlib\n",
    "import string\n",
    "import array\n",
    "import collections\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ed9bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Initialize PyTerrier (run once per session)\n",
    "if not pt.java.started():\n",
    "    pt.java.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a736e8",
   "metadata": {},
   "source": [
    "## üìÑ Dataset and Indexing\n",
    "\n",
    "We now load the IR dataset using `ir_datasets` and prepare it for use with PyTerrier by indexing its documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b26e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ir_datasets ir-measures scikit-learn dill pybind11 tqdm pympler python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c316e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: Dataset(id='antique/train', provides=['docs', 'queries', 'qrels'])\n",
      "Documents: 403666\n",
      "Queries: 2426\n",
      "Qrels (relevance judgments): 27422\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset using ir_datasets\n",
    "dataset = ir_datasets.load(\"antique/train\")\n",
    "\n",
    "# Print basic dataset info\n",
    "print(\"Dataset loaded:\", dataset)\n",
    "print(\"Documents:\", dataset.docs_count())\n",
    "print(\"Queries:\", dataset.queries_count())\n",
    "print(\"Qrels (relevance judgments):\", dataset.qrels_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f7969ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Please confirm you agree to the authors' data usage agreement found at <https://ciir.cs.umass.edu/downloads/Antique/readme.txt>\n",
      "[INFO] If you have a local copy of https://ciir.cs.umass.edu/downloads/Antique/antique-collection.txt, you can symlink it here to avoid downloading it again: /home/martina/.ir_datasets/downloads/684f7015aff377062a758e478476aac8\n",
      "[INFO] [starting] https://ciir.cs.umass.edu/downloads/Antique/antique-collection.txt\n",
      "[INFO] [finished] https://ciir.cs.umass.edu/downloads/Antique/antique-collection.txt: [00:29] [93.6MB] [3.17MB/s]\n",
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:08:24.181 [ForkJoinPool-3-worker-1] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (730691_1) - further warnings are suppressed\n",
      "15:11:39.044 [ForkJoinPool-3-worker-1] WARN org.terrier.structures.indexing.Indexer -- Indexed 2224 empty documents\n"
     ]
    }
   ],
   "source": [
    "# Create the directory one level above notebooks\n",
    "import os\n",
    "os.makedirs(\"../indexes\", exist_ok=True)\n",
    "\n",
    "# Set path for index\n",
    "index_path = \"../indexes/antique-index\"\n",
    "\n",
    "# Build the index if it doesn't already exist\n",
    "if not os.path.exists(os.path.join(index_path, \"data.properties\")):\n",
    "    indexer = pt.IterDictIndexer(index_path)\n",
    "    indexref = indexer.index(\n",
    "        ({\"docno\": doc.doc_id, \"text\": doc.text} for doc in dataset.docs_iter())\n",
    "    )\n",
    "else:\n",
    "    indexref = pt.IndexRef.of(index_path)\n",
    "\n",
    "# Load the index\n",
    "index = pt.IndexFactory.of(indexref)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd437b9d",
   "metadata": {},
   "source": [
    "## üîç Retrieval: TF-IDF and BM25\n",
    "\n",
    "We now create two retrieval pipelines: one based on the TF-IDF weighting scheme, and one on BM25. These are evaluated on the Vaswani dataset using standard IR metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2cfaf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11193/422261853.py:2: DeprecationWarning: Call to deprecated class BatchRetrieve. (use pt.terrier.Retriever() instead) -- Deprecated since version 0.11.0.\n",
      "  bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
      "/tmp/ipykernel_11193/422261853.py:3: DeprecationWarning: Call to deprecated class BatchRetrieve. (use pt.terrier.Retriever() instead) -- Deprecated since version 0.11.0.\n",
      "  tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
      "[INFO] [starting] https://ciir.cs.umass.edu/downloads/Antique/antique-train-queries.txt\n",
      "[INFO] [finished] https://ciir.cs.umass.edu/downloads/Antique/antique-train-queries.txt: [00:00] [137kB] [373kB/s]\n",
      "[INFO] [starting] https://ciir.cs.umass.edu/downloads/Antique/antique-train.qrel                \n",
      "[INFO] [finished] https://ciir.cs.umass.edu/downloads/Antique/antique-train.qrel: [00:01] [626kB] [624kB/s]\n",
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample queries:\n",
      "  query_id                                               text\n",
      "0  3097310  What causes severe swelling and pain in the kn...\n",
      "1  3910705  why don't they put parachutes underneath airpl...\n",
      "2   237390                how to clean alloy cylinder heads ?\n",
      "3  2247892                          how do i get them whiter?\n",
      "4  1078492                    What is Cloud 9 and 7th Heaven?\n",
      "Sample qrels:\n",
      "  query_id     doc_id  relevance iteration\n",
      "0  2531329  2531329_0          4        U0\n",
      "1  2531329  2531329_5          4        Q0\n",
      "2  2531329  2531329_4          3        Q0\n",
      "3  2531329  2531329_7          3        Q0\n",
      "4  2531329  2531329_6          3        Q0\n"
     ]
    }
   ],
   "source": [
    "# Create BM25 and TF-IDF retrieval pipelines\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "\n",
    "# Load queries and qrels (ground-truth relevance judgments)\n",
    "topics = dataset.queries_iter()\n",
    "qrels = dataset.qrels_iter()\n",
    "\n",
    "# Convert queries and qrels to DataFrames for use with PyTerrier\n",
    "topics_df = pd.DataFrame([t._asdict() for t in topics])\n",
    "qrels_df = pd.DataFrame([q._asdict() for q in qrels])\n",
    "\n",
    "# Preview query format\n",
    "print(\"Sample queries:\")\n",
    "print(topics_df.head())\n",
    "print(\"Sample qrels:\")\n",
    "print(qrels_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f3c0c",
   "metadata": {},
   "source": [
    "### üõ† Column Mapping for PyTerrier Compatibility\n",
    "\n",
    "To use custom `topics` and `qrels` DataFrames with `pt.Experiment`, you must rename the columns to match what PyTerrier expects:\n",
    "\n",
    "| Original Column | Renamed To | Reason                        |\n",
    "|------------------|-------------|-------------------------------|\n",
    "| `query_id`       | `qid`       | PyTerrier expects this field |\n",
    "| `text`           | `query`     | PyTerrier expects this field |\n",
    "| `doc_id`         | `docno`     | Matches index document field |\n",
    "| `relevance`      | `label`     | Required as relevance score  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f59732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert iterators to DataFrames\n",
    "topics_df = pd.DataFrame(dataset.queries_iter())\n",
    "qrels_df = pd.DataFrame(dataset.qrels_iter())\n",
    "\n",
    "# Rename only if necessary\n",
    "topics_df = topics_df.rename(columns={k: v for k, v in {\n",
    "    \"query_id\": \"qid\",\n",
    "    \"text\": \"query\"\n",
    "}.items() if k in topics_df.columns})\n",
    "\n",
    "qrels_df = qrels_df.rename(columns={k: v for k, v in {\n",
    "    \"query_id\": \"qid\",\n",
    "    \"doc_id\": \"docno\",\n",
    "    \"relevance\": \"label\"\n",
    "}.items() if k in qrels_df.columns})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d491ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       qid                                              query\n",
      "0  3097310  What causes severe swelling and pain in the knees\n",
      "1  3910705  why dont they put parachutes underneath airpla...\n",
      "2   237390                 how to clean alloy cylinder heads \n",
      "       qid      docno  label iteration\n",
      "0  2531329  2531329_0      4        U0\n",
      "1  2531329  2531329_5      4        Q0\n",
      "2  2531329  2531329_4      3        Q0\n"
     ]
    }
   ],
   "source": [
    "# Preprocess queries: strip punctuation that breaks Terrier parser\n",
    "topics_df[\"query\"] = topics_df[\"query\"].str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
    "\n",
    "# Optional preview\n",
    "print(topics_df.head(3))\n",
    "print(qrels_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e7bd7845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>R@5</th>\n",
       "      <th>R@10</th>\n",
       "      <th>R@15</th>\n",
       "      <th>R@20</th>\n",
       "      <th>R@30</th>\n",
       "      <th>R@100</th>\n",
       "      <th>R@200</th>\n",
       "      <th>R@500</th>\n",
       "      <th>R@1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.134620</td>\n",
       "      <td>0.285444</td>\n",
       "      <td>0.124418</td>\n",
       "      <td>0.159724</td>\n",
       "      <td>0.182997</td>\n",
       "      <td>0.200185</td>\n",
       "      <td>0.222592</td>\n",
       "      <td>0.292359</td>\n",
       "      <td>0.336036</td>\n",
       "      <td>0.395777</td>\n",
       "      <td>0.439749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.134642</td>\n",
       "      <td>0.285791</td>\n",
       "      <td>0.124607</td>\n",
       "      <td>0.160150</td>\n",
       "      <td>0.183631</td>\n",
       "      <td>0.200644</td>\n",
       "      <td>0.223307</td>\n",
       "      <td>0.293343</td>\n",
       "      <td>0.336536</td>\n",
       "      <td>0.396537</td>\n",
       "      <td>0.441141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name       map      ndcg       R@5      R@10      R@15      R@20  \\\n",
       "0  TF-IDF  0.134620  0.285444  0.124418  0.159724  0.182997  0.200185   \n",
       "1    BM25  0.134642  0.285791  0.124607  0.160150  0.183631  0.200644   \n",
       "\n",
       "       R@30     R@100     R@200     R@500    R@1000  \n",
       "0  0.222592  0.292359  0.336036  0.395777  0.439749  \n",
       "1  0.223307  0.293343  0.336536  0.396537  0.441141  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyterrier.measures import *\n",
    "\n",
    "results = pt.Experiment(\n",
    "    [tfidf, bm25],\n",
    "    topics_df,\n",
    "    qrels_df,\n",
    "    eval_metrics=[\"map\", \"ndcg\", \"recall\"],\n",
    "    names=[\"TF-IDF\", \"BM25\"]\n",
    ")\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e559b014",
   "metadata": {},
   "source": [
    "## üìä Retrieval Performance on ANTIQUE/test\n",
    "\n",
    "We evaluated two classic retrieval models ‚Äî **TF-IDF** and **BM25** ‚Äî on the ANTIQUE/test dataset using PyTerrier. Both models were run against the full document collection, and their performance was measured using standard IR metrics.\n",
    "\n",
    "### üîç Evaluation Metrics\n",
    "\n",
    "| Model   | MAP      | nDCG     | R@5     | R@10    | R@15    | R@20    | R@30    | R@100   | R@200   | R@500   | R@1000  |\n",
    "|---------|----------|----------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n",
    "| TF-IDF  | 0.134620 | 0.285444 | 0.12442 | 0.15972 | 0.18300 | 0.20018 | 0.22259 | 0.29236 | 0.33604 | 0.39578 | 0.43975 |\n",
    "| BM25    | 0.134642 | 0.285791 | 0.12461 | 0.16015 | 0.18363 | 0.20064 | 0.22331 | 0.29334 | 0.33654 | 0.39654 | 0.44114 |\n",
    "\n",
    "### üìå Observations\n",
    "\n",
    "- **BM25 slightly outperforms TF-IDF** across all metrics, particularly on deep recall levels like R@1000.\n",
    "- The performance difference is **marginal**, indicating both models behave similarly on this dataset.\n",
    "- These scores reflect the difficulty of **open-ended natural language queries** in the ANTIQUE dataset ‚Äî improvements would likely require semantic models (e.g., BERT-based retrieval).\n",
    "\n",
    "These baseline results provide a foundation for future comparison with more advanced neural or hybrid ranking approaches.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
